# å¿«é€Ÿå¼€å§‹ï¼šè®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼ˆ2å¼ GPUï¼‰

## ğŸ“‹ å·²åˆ›å»ºçš„é…ç½®æ–‡ä»¶

1. âœ… `recipes/accelerate_config/deepspeed_zero3_2gpu.yaml` - 2å¼ GPUçš„DeepSpeedé…ç½®
2. âœ… `recipes/mistral-7b-deita/teacher_sft.yaml` - SFTè®­ç»ƒé…ç½®
3. âœ… `recipes/mistral-7b-deita/teacher_dpo.yaml` - DPOè®­ç»ƒé…ç½®

## ğŸš€ æ‰§è¡Œæ­¥éª¤

### æ­¥éª¤1ï¼šè®­ç»ƒREF Teacherï¼ˆSFTï¼‰

```bash
cd ~/ADPA

CUDA_VISIBLE_DEVICES=0,1 \
ACCELERATE_LOG_LEVEL=info \
DS_SKIP_CUDA_CHECK=1 \
python -m accelerate.commands.launch \
  --config_file recipes/accelerate_config/deepspeed_zero3_2gpu.yaml \
  scripts/run_sft.py \
  recipes/mistral-7b-deita/teacher_sft.yaml
```

**è¾“å‡º**ï¼š`~/ADPA/data/mistral-7b-deita/ref_teacher`

### æ­¥éª¤2ï¼šè®­ç»ƒDPO Teacher

```bash
cd ~/ADPA

CUDA_VISIBLE_DEVICES=0,1 \
ACCELERATE_LOG_LEVEL=info \
DS_SKIP_CUDA_CHECK=1 \
python -m accelerate.commands.launch \
  --config_file recipes/accelerate_config/deepspeed_zero3_2gpu.yaml \
  scripts/run_distill_dpo.py \
  recipes/mistral-7b-deita/teacher_dpo.yaml
```

**è¾“å‡º**ï¼š`~/ADPA/data/mistral-7b-deita/dpo_teacher`

## âš™ï¸ å…³é”®ä¿®æ”¹è¯´æ˜

### 1. GPUæ•°é‡è°ƒæ•´
- **åŸé…ç½®**ï¼š8å¼ GPU (`CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7`)
- **æ–°é…ç½®**ï¼š2å¼ GPU (`CUDA_VISIBLE_DEVICES=0,1`)
- **Accelerateé…ç½®**ï¼š`num_processes: 2`

### 2. Batch Sizeè°ƒæ•´
- **åŸé…ç½®**ï¼š`per_device_batch_size=1 Ã— 8å¡ Ã— gradient_accumulation=16 = æœ‰æ•ˆbatch=128`
- **æ–°é…ç½®**ï¼š`per_device_batch_size=1 Ã— 2å¡ Ã— gradient_accumulation=64 = æœ‰æ•ˆbatch=128`
- **ä¿æŒç›¸åŒçš„æœ‰æ•ˆbatch size**ï¼Œç¡®ä¿è®­ç»ƒæ•ˆæœä¸€è‡´

### 3. è·¯å¾„é…ç½®
- **æ¨¡å‹è·¯å¾„**ï¼š`/home/yunbokun/Models/Teacher/mistralai/Mistral-7B-v0.1`
- **SFTæ•°æ®é›†**ï¼š`/home/yunbokun/Datasets/sft/deita-10k-v0-sft`
- **DPOæ•°æ®é›†**ï¼š`/home/yunbokun/Datasets/preference alignment/dpo-mix-7k`
- **è¾“å‡ºç›®å½•**ï¼š`~/ADPA/data/mistral-7b-deita/`

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ•°æ®é›†æ ¼å¼**ï¼š
   - SFTæ•°æ®é›†éœ€è¦ `train_sft` å’Œ `test_sft` split
   - DPOæ•°æ®é›†éœ€è¦ `train` splitï¼ŒåŒ…å« `chosen` å’Œ `rejected` å­—æ®µ

2. **å¦‚æœæ•°æ®é›†è·¯å¾„ä¸å¯¹**ï¼š
   - æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ `dataset_mixer` éƒ¨åˆ†
   - å¯ä»¥ä½¿ç”¨HuggingFace Hubè·¯å¾„ï¼ˆå¦‚ `HuggingFaceH4/deita-10k-v0-sft`ï¼‰

3. **æ˜¾å­˜ä¸è¶³**ï¼š
   - å‡å° `max_seq_length` æˆ– `max_length`
   - å¢åŠ  `gradient_accumulation_steps`

4. **è®­ç»ƒæ—¶é—´**ï¼š
   - SFTï¼š3ä¸ªepochï¼Œå¯èƒ½éœ€è¦æ•°å°æ—¶
   - DPOï¼š1ä¸ªepochï¼Œç›¸å¯¹è¾ƒå¿«

## âš ï¸ å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

### é”™è¯¯1: CUDAå…¼å®¹æ€§é—®é¢˜ï¼ˆBlackwell GPUï¼‰

å¦‚æœé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
NVIDIA RTX PRO 6000 Blackwell Workstation Edition with CUDA capability sm_120 is not compatible
torch.distributed.DistBackendError: NCCL error
```

**åŸå› **ï¼šBlackwellæ¶æ„ï¼ˆsm_120ï¼‰éœ€è¦PyTorch 2.5+ç‰ˆæœ¬æ”¯æŒã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡çº§PyTorchåˆ°æ”¯æŒBlackwellçš„ç‰ˆæœ¬
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# æˆ–ä½¿ç”¨conda
conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia
```

è¯¦ç»†è¯´æ˜è¯·å‚è€ƒï¼š`TROUBLESHOOTING.md`

### é”™è¯¯2: trust_remote_codeè­¦å‘Š

æ­¤é—®é¢˜å·²ä¿®å¤ã€‚å¦‚æœä»çœ‹åˆ°è­¦å‘Šï¼Œè¯·ç¡®ä¿ä½¿ç”¨æœ€æ–°ä»£ç ã€‚

## ğŸ“ è¯¦ç»†è¯´æ˜

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒï¼š
- `REPRODUCTION_STEPS.md` - å®Œæ•´å¤ç°æ­¥éª¤
- `TROUBLESHOOTING.md` - é”™è¯¯æ’æŸ¥æŒ‡å—
